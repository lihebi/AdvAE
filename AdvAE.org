#+TITLE: Adversarially trained denoising Auto-Encoder (AdvAE)
#+LATEX_CLASS: nips
#+LATEX_HEADER: \usepackage[export]{adjustbox}

# These two combo can make larger width image while centered
# #+ATTR_LATEX: :width 1.2\linewidth,center

* Introduction

* test result data format design

#+BEGIN_SRC json
{
"no attack CNN": 0.996,
"no attack AE": 0.991,
"num_samples": 100,
"epsilon-exp":{
    "header": ["ep", "FGSM", "PGD", "Hop"],
    "data": [[0.02, 1.0, 1.0],
             [0.04, 1.0, 1.0]
             ...
             [...]]},

}
#+END_SRC

* Experiment Logs

- 0831_0AM
- 0831_4AM: This is 30 epoch, without setting y in PGD. I need to
  rerun the tests for setting y in PGD
- 0831_2PM: This is 10 epoch, with setting y in PGD. The ItAdv
  versions do not typically converge, but can sometimes converge
  randomly.
- 0901_8PM: 10 epoch CIFAR exp, without setting y in PGD training.

All the models before 0901 8PM (inclusive) have a defect of
ItAdvCNN. For diferent AE model, this is overwritten, but should not
have.

- 0902_11AM: 20 epoch CIFAR, with setting y during training.
- 0903_2AM: 50 epoch CIFAR, with setting y during training.

I'm going to delete all MNIST models and rerun with:
- 30 epoch (same as before)
- with setting y during training
- All MNIST using CNN1AE instead of CNN3AE by default

Also, I'm running more CIFAR models using 30 epochs this time.


* TODOs for AAAI
** STARTED [#A] new results on transfer and ensemble
** STARTED [#A] Get defgan into plot
** Performance-wise
- Can I use a smaller number of iterations of PGD during training?

** DONE The target tensor in PGD
   CLOSED: [2019-09-03 Tue 03:13]
*** DONE [#A] Testing
    CLOSED: [2019-09-03 Tue 03:13]

Use the new PGD attack with y supplied

*** DONE [#A] Training
    CLOSED: [2019-09-03 Tue 03:13]

Use y option in FGSM/PGD during training.

Why setting y giving bad results, not converging?

** STARTED [#A] CIFAR experiments
The CIFAR experiments are not good for proper PGD attack (with
target tensor). Thus, I'm considering two reasons:
- not enough epoch
- not using the proper PGD with target during training
*** difference between mnist_challenge and cifar_challenge

- data augmentation for adv training
- weight decay
- NOT using C2 loss

**** Additional experiments in Madry's paper

- plot training loss and validation loss during training
- in epsilon plot, show the training epsilon
- show l2 norm epsilon plot. Madry's paper didn't mention which l2 it
  is using.
- show adversarial with different number of steps

- 40, 0.01, 0.3 for training, 40, 100 for testing
  - no attack: 98.8
  - 40: 93.2%
  - 100: 89.3%
- 7, 2, 8 for training, 7, 20 for testing 
  - no attack: 87.3
  - 50.0%
  - 45.8%

*** I was using 0.3 to test cifar
I should have used 8/255 instead.

*** DONE I'm exploring these.
    CLOSED: [2019-09-02 Mon 11:20]
- using 20 epoch
- using PGD with targets during training

*** TODO Next, I probably
- increasing epochs
- training using PGD with targets in MNIST exp

*** TODO [#A] Remove dunet in CIFAR

** TODO [#A] Investigate CNN3AE problems
It seems that CNN3AE the last layer needs some special treatment
('sampling' not to be 'same'). This also does not work on CIFAR model
in terms of dimension match.

Thus, I'm going to either
- investigate why CNN3AE has such problem, or
- remove CNN3AE entirely

*** replace CNN3AE with CNN1AE
*** model all (all 2/4/6-layer CNN) using get_wideae lambda function

*** TODO [#A] Try not bottleneck AE



** [#C] test x+delta in regularization term
** DONE Better result formatting
   CLOSED: [2019-08-31 Sat 01:06]
** [#C] Try C2 loss
** DONE The #param might be interesting
   CLOSED: [2019-08-31 Sat 01:07]
The AE parameter is usually much smaller than the CNN parameter

** Inference time comparison
** [#B] BPDA on AdvAE

** [#C] Try different parameters of PGD *during* training
** [#C] PuVAE


** CANCELED How to run faster? mini-batch?
   CLOSED: [2019-08-29 Thu 16:11]
I'm using up to 30 epochs.

** Tables

Old table:

| attacks  | no def acc | black-box | white-box | oblivious |
|----------+------------+-----------+-----------+-----------|
| FGSM     |            |           |           |           |
| PGD      |            |           |           |           |
| CW       |            |           |           |           |
| Hop      |            |           |           |           |

To remove the black-box and oblivious attack confusion:

| attacks         | no def acc | AdvAE | DefenseGAN | HGD | PureVAE | AdvTrain | AdvTrain+AE |   |
|-----------------+------------+-------+------------+-----+---------+----------+-------------+---|
| traininig time  |            |       |            |     |         |          |             |   |
| inference time  |            |       |            |     |         |          |             |   |
| transferable    |            | Yes   | Yes        | Yes | Yes     | No       | No          |   |
|-----------------+------------+-------+------------+-----+---------+----------+-------------+---|
| FGSM            |            |       |            |     |         |          |             |   |
| PGD obli        |            |       |            |     |         |          |             |   |
| PGD             |            |       |            |     |         |          |             |   |
| CW              |            |       |            |     |         |          |             |   |
| Hop (black-box) |            |       |            |     |         |          |             |   |

I might want to remove CW because:
- l2 distance seems not working for all defenses
- CW is slow

Also, the epsilon table should supercede this table. So I can remove
this table entirely. If I really want to a table to show the numbers,
pick a epsilon=0.3.

Transferability

| attacks         | no def acc | X/X | X/A | X/B | X/C | X/D | Classification / Detection |
|-----------------+------------+-----+-----+-----+-----+-----+----------------------------|
| FGSM            |            |     |     |     |     |     |                            |
| PGD             |            |     |     |     |     |     |                            |
| CW              |            |     |     |     |     |     |                            |
| Hop (black-box) |            |     |     |     |     |     |                            |

Ensemble

| attacks         | no def acc | X/X | X/A | X/B | X/C | X/D |
|-----------------+------------+-----+-----+-----+-----+-----+
| FGSM            |            |     |     |     |     |     |
| PGD             |            |     |     |     |     |     |
| CW              |            |     |     |     |     |     |
| Hop (black-box) |            |     |     |     |     |     |


*** Epsilon table
Different distortion (use a figure)

| attacks  | no def acc | epsilon = 0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| FGSM     |            |             |     |     |     |     |     |
| PGD      |            |             |     |     |     |     |     |
| CW       |            |             |     |     |     |     |     |
| Hop      |            |             |     |     |     |     |     |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| defense  |            |             |     |     |     |     |     |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| AdvAE    |            |             |     |     |     |     |     |
| HGD      |            |             |     |     |     |     |     |
| PureVAE  |            |             |     |     |     |     |     |
| AdvTrain |            |             |     |     |     |     |     |

*** Different AE

| attacks         | no def acc | d1, w1 | d1 w2 | d2 w1 | d2 w2 | dunet |
|-----------------+------------+--------+-------+-------+-------+-------|
| # params        |            |        |       |       |       |       |
|-----------------+------------+--------+-------+-------+-------+-------|
| FGSM            |            |        |       |       |       |       |
| PGD             |            |        |       |       |       |       |
| CW              |            |        |       |       |       |       |
| Hop (black-box) |            |        |       |       |       |       |

*** lambda
Probably a figure for this.

| attacks         | no def acc | lambda = 0 | 0.2 | 0.5 | 1 | 2 | 5 |
|-----------------+------------+------------+-----+-----+---+---+---|
| FGSM            |            |            |     |     |   |   |   |
| PGD             |            |            |     |     |   |   |   |
| CW              |            |            |     |     |   |   |   |
| Hop (black-box) |            |            |     |     |   |   |   |

*** Training process plot
Training loss, validation loss, validation accuracy.



* New TODOs
** DONE BPDA
   CLOSED: [2019-07-30 Tue 18:00]
** TODO Transferability on CIFAR models
** TODO performance on CIFAR
** DONE other blackbox
   CLOSED: [2019-08-27 Tue 10:58]
** TODO simplify base models
- e.g. remove dropout, remove unused FC and CNN layers in both base
  models and AE model.
- also do sth. about dunet and CIFAR


* Possible problems
Probably:
- K.learning_phase (3684993)
- PGD stop gradients (9c21e64)
- add dunet model (55b34b5)

No:
- setupFC (3684993)
- AE pretraining
* Approach

** Loss
We use the addition of four loss terms as loss function.

** Training
4. (optional) alternatively train denoiser and CNN, so that
the precision is still good. This may have equivalent effect as
training denoiser using high level feature guidance

4.1 FIXME probably also consider training for from clean x to x and to
logits, as that is the whole model

* Implementations notes                                            :noexport:
** DONE debug training time
   CLOSED: [2019-04-30 Tue 17:42]
** DONE inconsistency problems
   CLOSED: [2019-05-07 Tue 11:42]

- standalone attacks vs. integrated (in class as method) attacks: running time, accuracy
- accuracy computation inconsistency

** DONE CW visual result
   CLOSED: [2019-05-07 Tue 11:41]
** DONE add postadv baseline
   CLOSED: [2019-05-07 Tue 11:41]

** I want to try not pre-training auto encoder
** https://www.robust-ml.org/

** Defense GAN break
** Auto encoder (pre)-training without noise
** Resnet 56/110

** Other CNN structure
*** VGG
*** Wide Resnet
*** Fully convolutional network

** More dataset
*** CIFAR exp
*** Fashion MNIST
*** MNIST
*** Large-scale CelebFaces Attributes (CelebA) Dataset
Seems to be human face, maybe commonly used in generative networks.

** Train AE using classification logits
*** try learning rate decay
*** try data augmentation
*** TODO understand Unet
- Understand the unet, what to use (addition?) as output.
- test training dunet using only noisy term
- try dunet without pre-training. The pretraining of dunet is weird:
  the accuracy reaches 85 very soon, but it still trains a lot of
  epochs. If overfitting it at this time, it might have negative
  effects on adv training step. So maybe just directly do adv training
  with C0 or C2 as a loss term. I probably have to use a C0/C2 term anyway.
*** test all the different loss terms
only if the dunet is not giving promising results.
*** integrate this with adv training

** Adv training of GANs?
** Compare with adv training
- show that the performance drop is not significant.
*** Try cifar10 challenge code
- model
- data augmentation
- PGD with their iteration
- CW by using CW loss function but PGD iterations

** investigate not only accuracy, but also confidence
** save keras training history


* Other Ideas                                                      :noexport:
** Ensemble
** random CNN as task


** TODO Add data augmentation during AE and adv training?
** Add noise, and then add PGD, and then use in training
** TODO add a little CW into PGD training
** unsuperwisely train AE
Do not use image data at all. Generate a data, assign random labels,
train the network. The network might have random guessing for
test/validation data, but can be 100% at training data. 

Using this network, train the AE.

* Additional Experiments
** DONE Black box substitute model accuracy
   CLOSED: [2019-05-21 Tue 11:33]
** DONE Model transfer
   CLOSED: [2019-05-21 Tue 12:15]
*** DONE Simple CNNs for MNIST
    CLOSED: [2019-05-16 Thu 00:28]
*** CANCELED VGG for CIFAR
    CLOSED: [2019-05-21 Tue 01:12]
*** DONE DenseNet
    CLOSED: [2019-05-21 Tue 11:33]
- original torch https://github.com/liuzhuang13/DenseNet
- keras implementation: https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet
** DONE DefenseGAN break
   CLOSED: [2019-05-21 Tue 01:12]
** DONE Test using all test data
   CLOSED: [2019-05-21 Tue 11:33]
instead of random 100

** TODO try other auto encoders other than dunet

* Nice-to-have experiments

** TODO Adv train both AE and CNN
** TODO use data augmentation during adv training

* Experiment

** TODO train on several digits, leave out 2
Do it on both AdvAE and adv training. This may even show better
performance than adv training.


** DONE CIFAR
   CLOSED: [2019-05-15 Wed 23:07]
*** TODO resnet AE design
*** TODO add high level xent when pretraining AE
*** TODO VGG etc for CIFAR
Because training AE for CIFAR is pretty hard
** TODO Imagenet

** TODO compare with other defenses
*** DONE Adv training
    CLOSED: [2019-05-15 Wed 23:07]
*** HAE: high-level feature guided AE
**** one iteration high adv prove it fail on white box
  - oblivious
  - unet
*** Ensemble method

*** TODO Compare to generative models
analyze the difference, pros and cons, compared to generative methods.
- Defense-GAN
- PuVAE


** AdvAE against different attacks
- test whether this works for CW

PostNoisy_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.13 |
| PGD     |     0.94 |          5.20 |
| JSMA    |     0.89 |          4.54 |
| CW      |     0.22 |          2.48 |

AdvAE (default) (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.10 |
| PGD     |     0.91 |          5.29 |
| JSMA    |     0.72 |          4.82 |
| CW      |     0.73 |           0.9 |

Post_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.97 |          6.10 |
| PGD     |     0.96 |          5.10 |
| JSMA    |     0.93 |          4.20 |
| CW      |     0.57 |           0.9 |

*** TODO we need a total accuracy table

|      | AdvAE | PostNoisy_Adv | AdvAE (10 epoch) |
|------+-------+---------------+------------------|
| FGSM |       |               |                  |
| PGD  |       |               |                  |
| JSMA |       |               |                  |
| CW   |       |               |                  |

*** TODO run full training instead of 10 epochs

** TODO AdvAE transferability to other CNN architectures

- test whether this works for different CNN structure out of box, or
  even FC

different CNN architecture:
- different kernel filter size
- different number of layers
- different activation functions
- different pooling size and scheme
- residual connections
- dropout

*** TODO Ensemble training
- ensemble different CNN architecture. I suspect that the rec terms
  actually act as regularizer for different CNNs. We'll see.

How to ensemble? Create many CNN layers. When training, add all loss
terms of different CNNs together.

** Ensemble different attack parameters
Or random

** TODO Analyze of different loss terms
- [ ] plot the training and loss
- analyze how the different loss terms work. Even if the loss does
  not seem to decrease, it might act as a regularizer. Try removing it
  in the train step, and observe if that term increases and goes out
  of control.
- see whether it is necessary any more to use high layers of CNN.
- add weights to the different terms, and apply weight decay

|   | term1 | term2 | term3 | term4 | adv accuracy |
|---+-------+-------+-------+-------+--------------|
|   | Y     |       |       |       |              |
|   |       | Y     |       |       |              |
|   |       |       | Y     |       |              |
|   |       |       |       | Y     |              |
|---+-------+-------+-------+-------+--------------|
|   | Y     | Y     |       |       |              |
|   | Y     |       | Y     |       |              |


default model
- =AdvAE=

stand alone model (not likely to work)
- =Post=

combine witth adv loss
- =Post_Adv=
- =Noisy_Adv=
- =PostNoisy_Adv=

add clean models
- =CleanAdv=
- =Post_CleanAdv=
- =Noisy_CleanAdv=
- =PostNoisy_CleanAdv=

high-level guided models
- High
- =High_Adv=
- =PostHigh_Adv=

** Denoiser capacity
- investigate whether increasing denoiser capacity helps with defense
  against CW
- test whether using FC instead of AE can also achieve similar results
** visualize what the denoiser is doing on adv images
** TODO visualize and analyze the successful attacks

** TODO PostAdv
- add adv noise at CNN input, after AE
- AE acts as a anti-adv example generator

* Result

MNIST (A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.98 |            |            0.98 |          |          0.97 |                   0.99 |        |
| CW      |         0. |       0.97 |            0.81 |     0.96 |            0. |                   0.86 |   0.55 |
| FGSM    |       0.16 |       0.95 |            0.95 |     0.98 |          0.24 |                   0.97 |        |
| PGD     |       0.01 |       0.96 |            0.94 |     0.99 |          0.02 |                   0.95 |        |

F-MNIST (A2)
| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.94 |            |            0.72 |          |          0.70 |                   0.83 |        |
| CW      |          0 |       0.72 |            0.45 |     0.74 |           0.0 |                   0.66 |        |
| FGSM    |       0.07 |       0.80 |            0.81 |     0.80 |          0.32 |                   0.83 |        |
| PGD     |       0.03 |       0.78 |            0.73 |     0.96 |          0.21 |                   0.69 |        |

F-MNIST (C0 A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.94 |            |            0.82 |          |          0.70 |                   0.83 |        |
| CW      |          0 |       0.81 |            0.52 |     0.74 |           0.0 |                   0.66 |        |
| FGSM    |       0.07 |       0.76 |            0.72 |     0.80 |          0.32 |                   0.83 |        |
| PGD     |       0.03 |       0.78 |            0.63 |     0.96 |          0.21 |                   0.69 |        |

AdvAE Cifar10 (C0 A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN  |
|---------+------------+------------+-----------------+----------+---------------+------------------------+---------|
| clean   |       0.89 |            |            0.61 |          |          0.82 |                   0.67 |         |
| CW      |          0 |       0.62 |            0.01 |     0.82 |            0. |                     0. |         |
| FGSM    |       0.17 |       0.62 |            0.52 |     0.84 |          0.15 |                   0.48 |         |
| PGD     |       0.07 |       0.61 |            0.46 |     0.83 |          0.11 |                   0.43 |         |

Notes:
- HGD: B2 loss
- AdvAE MNIST: A2 loss
- AdvAE Cifar10: C0_A2 loss
- adv training: IdentityModel


#+TITLE: Implementation notes
* References
- a pytorch adv training library from MadryLab https://github.com/MadryLab/robustness

* General
** Data preparation
- dataset! https://github.com/JuliaML/MLDatasets.jl
  - pattern https://github.com/JuliaML/MLDataPattern.jl

- Images.jl: https://github.com/JuliaImages/Images.jl
- ImageFiltering.jl: where padarray locates, exported in Images namespace
  https://github.com/JuliaImages/ImageFiltering.jl

- Augmentor.jl, a very nice library, but not on Julia 1.0
  https://github.com/Evizero/Augmentor.jl
  - update to 1.x PR: https://github.com/Evizero/Augmentor.jl/pull/29

- Metalhead.jl/ https://github.com/FluxML/Metalhead.jl/
  - Metalhead.jl has training code for ImageNet
    https://github.com/FluxML/Metalhead.jl/blob/sf/training/src/preprocessing.jl

* AdvAE

** STARTED MNIST testing and logging
*** TODO AdvAE
**** auto encoder
***** Dunet
*** HGD

** CIFAR10 testing

** STARTED GAN
*** GAN
*** DefenseGAN

** attacks
*** TODO CW
*** TODO black-box substitute model
*** TODO BoundaryAttack, BAPP
*** TODO BPDA

* robust training
** STARTED cifar10

*** DONE select the best resnet model
    CLOSED: [2019-11-16 Sat 07:06]
- first match the results from the papers
- Possibly consider DenseNet

**** DONE data augmentation
     CLOSED: [2019-11-16 Sat 07:06]
**** CANCELED -mean/variance and handle adv epsilon bounds
     CLOSED: [2019-11-16 Sat 07:06]

**** TODO densenet

*** STARTED adv train it

*** mix clean data
*** dynamic attacking strength

** TODO imagenet

** plots

*** DONE show problem of itadv
    CLOSED: [2019-11-14 Thu 11:21]

plot different learning rate onto loss/acc

*** TODO visualize the gradient surface of mixing clean data

*** DONE pretrianed CNN with lambda=0
    CLOSED: [2019-11-14 Thu 17:13]
*** TODO nat+clean 1:1

*** compare with dynamic data mixing
plot different learning rate onto loss/acc, both approach
*** compare dynamic data mixing with dynamic attacking strength

- Dy-Mix
- Dy-Attack

1. the number of steps to converge should be the same
2. the time/step compares three approaches:
   - Dy-Mix, Dy-Attack, It-Adv

** dynamic data mixing

- a good article for dynamic learning rate https://www.jeremyjordan.me/nn-learning-rate/
- visualize and show the gradient surface, such as https://arxiv.org/abs/1712.09913
- learning rate paper: Cyclical Learning Rates for Training Neural Networks

*** Some questions

- when loss decreases, the input gradient of loss is smaller, then the learning
  rate should increase?

- should the lambda ratio of nat and adv sum to 1?

** TODO dynamic attacking strength

*** cifar10 resnet model
https://github.com/tensorflow/models/blob/master/official/vision/image_classification/resnet_cifar_model.py

*** adapt natural train and adv train for cifar models
*** implement dy-attack
*** imagenet
*** compare with free_train
*** compare with 1:1 data mixing


* DONE-list

** DONE CIFAR10 models
   CLOSED: [2019-11-16 Sat 02:20]

*** DONE Verifying implementation
    CLOSED: [2019-11-16 Sat 02:20]

- keras resnet https://keras.io/examples/cifar10_resnet/
- wide resnet pytorch:
  https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide_resnet.py
- metalhead resnet:
  https://github.com/FluxML/Metalhead.jl/blob/master/src/resnet.jl
- resnet from He: https://github.com/KaimingHe/deep-residual-networks#models
- deprecated official torch code from FAIR (but I found good)
  https://github.com/facebookarchive/fb.resnet.torch
- wide resnet official:
  https://github.com/szagoruyko/wide-residual-networks/blob/master/models/wide-resnet.lua

*** DONE learning rate schedule/decay
    CLOSED: [2019-11-16 Sat 02:20]
*** CANCELED early stopping
    CLOSED: [2019-11-13 Wed 16:17]


*** DONE batchnorm layer
    CLOSED: [2019-10-31 Thu 16:03]
*** DONE ResNet
    CLOSED: [2019-10-31 Thu 12:15]
*** CANCELED VGG
    CLOSED: [2019-10-31 Thu 12:15]

** TODO adversarial attacks
https://github.com/jaypmorgan/Adversarial.jl

*** DONE PGD
    CLOSED: [2019-11-01 Fri 16:27]
*** DONE FGSM
    CLOSED: [2019-11-01 Fri 16:27]


** CANCELED fix the local package version problem
   CLOSED: [2019-11-02 Sat 13:28]


** DONE @progress
   CLOSED: [2019-10-17 Thu 16:17]

ProgressMeter.jl https://github.com/timholy/ProgressMeter.jl

Very easy to use:

#+BEGIN_SRC julia
@showprogress 1 "Computing..." for i in 1:50
    sleep(0.1)
end
#+END_SRC

** DONE adversarial training
   CLOSED: [2019-11-13 Wed 16:16]
*** DONE itadvtrain
    CLOSED: [2019-11-01 Fri 16:27]

**** DONE convergency problem
     CLOSED: [2019-11-13 Wed 16:16]
There seems to be some problems: when directly using 20-PGD or 40-PGD, it does
not converge. Two ways:
- use 7-PGD, then 20-PGD, then 40-PGD. This seems to be the best strategy
- use 40-PGD directly, but train with both adv_x and x
- first train clean CNN for 1 epoch, then adv train
- weights initialization and regularization?

**** DONE consistency with python code
     CLOSED: [2019-11-13 Wed 16:16]
- speed seems to be a lot slower
- accuracy does not seem to be equal at each epoch
- convergency (or not) rate
- the final performance, 40-iter PGD, 0.8033, while should be 0.95

I'm going to use docker container to run the python code. For that I'd build a
machine with VNC support, via either:
- build ontop of tf official images
- see how tf official images are built, and build on top of ubuntu from scratch
** DONE tensorboard support
   CLOSED: [2019-11-14 Thu 10:02]
- tensorboard logger: https://github.com/PhilipVinc/TensorBoardLogger.jl/
  - or possibly: https://github.com/zenna/Tensorboard.jl

Install tensorflow:

#+begin_example
pip install --user tensorflow==1.15
#+end_example

The tensorflow package should install tensorboard. If not:

#+begin_example
pip install --user tensorboard==1.15
#+end_example


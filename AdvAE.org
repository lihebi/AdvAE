#+TITLE: Adversarially trained denoising Auto-Encoder (AdvAE)
#+LATEX_CLASS: nips
#+LATEX_HEADER: \usepackage[export]{adjustbox}

# These two combo can make larger width image while centered
# #+ATTR_LATEX: :width 1.2\linewidth,center

* Introduction

* test result data format design

#+BEGIN_SRC json
{
"no attack CNN": 0.996,
"no attack AE": 0.991,
"num_samples": 100,
"epsilon-exp":{
    "header": ["ep", "FGSM", "PGD", "Hop"],
    "data": [[0.02, 1.0, 1.0],
             [0.04, 1.0, 1.0]
             ...
             [...]]},

}
#+END_SRC


* TODOs for AAAI
** test x+delta in regularization term
** Better result formatting
** Remove dunet in CIFAR
** The #param might be interesting
The AE parameter is usually much smaller than the CNN parameter

** Tables

Old table:

| attacks  | no def acc | black-box | white-box | oblivious |
|----------+------------+-----------+-----------+-----------|
| FGSM     |            |           |           |           |
| PGD      |            |           |           |           |
| CW       |            |           |           |           |
| Hop      |            |           |           |           |

To remove the black-box and oblivious attack confusion:

| attacks         | no def acc | AdvAE | DefenseGAN | HGD | PureVAE | AdvTrain | AdvTrain+AE |   |
|-----------------+------------+-------+------------+-----+---------+----------+-------------+---|
| traininig time  |            |       |            |     |         |          |             |   |
| inference time  |            |       |            |     |         |          |             |   |
| transferable    |            | Yes   | Yes        | Yes | Yes     | No       | No          |   |
|-----------------+------------+-------+------------+-----+---------+----------+-------------+---|
| FGSM            |            |       |            |     |         |          |             |   |
| PGD obli        |            |       |            |     |         |          |             |   |
| PGD             |            |       |            |     |         |          |             |   |
| CW              |            |       |            |     |         |          |             |   |
| Hop (black-box) |            |       |            |     |         |          |             |   |

I might want to remove CW because:
- l2 distance seems not working for all defenses
- CW is slow

Also, the epsilon table should supercede this table. So I can remove
this table entirely. If I really want to a table to show the numbers,
pick a epsilon=0.3.

Transferability

| attacks         | no def acc | X/X | X/A | X/B | X/C | X/D | Classification / Detection |
|-----------------+------------+-----+-----+-----+-----+-----+----------------------------|
| FGSM            |            |     |     |     |     |     |                            |
| PGD             |            |     |     |     |     |     |                            |
| CW              |            |     |     |     |     |     |                            |
| Hop (black-box) |            |     |     |     |     |     |                            |

Ensemble

| attacks         | no def acc | X/X | X/A | X/B | X/C | X/D |
|-----------------+------------+-----+-----+-----+-----+-----+
| FGSM            |            |     |     |     |     |     |
| PGD             |            |     |     |     |     |     |
| CW              |            |     |     |     |     |     |
| Hop (black-box) |            |     |     |     |     |     |


*** Epsilon table
Different distortion (use a figure)

| attacks  | no def acc | epsilon = 0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| FGSM     |            |             |     |     |     |     |     |
| PGD      |            |             |     |     |     |     |     |
| CW       |            |             |     |     |     |     |     |
| Hop      |            |             |     |     |     |     |     |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| defense  |            |             |     |     |     |     |     |
|----------+------------+-------------+-----+-----+-----+-----+-----|
| AdvAE    |            |             |     |     |     |     |     |
| HGD      |            |             |     |     |     |     |     |
| PureVAE  |            |             |     |     |     |     |     |
| AdvTrain |            |             |     |     |     |     |     |

*** Different AE

| attacks         | no def acc | d1, w1 | d1 w2 | d2 w1 | d2 w2 | dunet |
|-----------------+------------+--------+-------+-------+-------+-------|
| # params        |            |        |       |       |       |       |
|-----------------+------------+--------+-------+-------+-------+-------|
| FGSM            |            |        |       |       |       |       |
| PGD             |            |        |       |       |       |       |
| CW              |            |        |       |       |       |       |
| Hop (black-box) |            |        |       |       |       |       |

*** lambda
Probably a figure for this.

| attacks         | no def acc | lambda = 0 | 0.2 | 0.5 | 1 | 2 | 5 |
|-----------------+------------+------------+-----+-----+---+---+---|
| FGSM            |            |            |     |     |   |   |   |
| PGD             |            |            |     |     |   |   |   |
| CW              |            |            |     |     |   |   |   |
| Hop (black-box) |            |            |     |     |   |   |   |

*** Training process plot
Training loss, validation loss, validation accuracy.



* New TODOs
** DONE BPDA
   CLOSED: [2019-07-30 Tue 18:00]
** TODO Transferability on CIFAR models
** TODO performance on CIFAR
** DONE other blackbox
   CLOSED: [2019-08-27 Tue 10:58]
** TODO simplify base models
- e.g. remove dropout, remove unused FC and CNN layers in both base
  models and AE model.
- also do sth. about dunet and CIFAR


* Possible problems
Probably:
- K.learning_phase (3684993)
- PGD stop gradients (9c21e64)
- add dunet model (55b34b5)

No:
- setupFC (3684993)
- AE pretraining
* Approach

** Loss
We use the addition of four loss terms as loss function.

** Training
4. (optional) alternatively train denoiser and CNN, so that
the precision is still good. This may have equivalent effect as
training denoiser using high level feature guidance

4.1 FIXME probably also consider training for from clean x to x and to
logits, as that is the whole model

* Implementations notes                                            :noexport:
** DONE debug training time
   CLOSED: [2019-04-30 Tue 17:42]
** DONE inconsistency problems
   CLOSED: [2019-05-07 Tue 11:42]

- standalone attacks vs. integrated (in class as method) attacks: running time, accuracy
- accuracy computation inconsistency

** DONE CW visual result
   CLOSED: [2019-05-07 Tue 11:41]
** DONE add postadv baseline
   CLOSED: [2019-05-07 Tue 11:41]

** I want to try not pre-training auto encoder
** https://www.robust-ml.org/

** Defense GAN break
** Auto encoder (pre)-training without noise
** Resnet 56/110

** Other CNN structure
*** VGG
*** Wide Resnet
*** Fully convolutional network

** More dataset
*** CIFAR exp
*** Fashion MNIST
*** MNIST
*** Large-scale CelebFaces Attributes (CelebA) Dataset
Seems to be human face, maybe commonly used in generative networks.

** Train AE using classification logits
*** try learning rate decay
*** try data augmentation
*** TODO understand Unet
- Understand the unet, what to use (addition?) as output.
- test training dunet using only noisy term
- try dunet without pre-training. The pretraining of dunet is weird:
  the accuracy reaches 85 very soon, but it still trains a lot of
  epochs. If overfitting it at this time, it might have negative
  effects on adv training step. So maybe just directly do adv training
  with C0 or C2 as a loss term. I probably have to use a C0/C2 term anyway.
*** test all the different loss terms
only if the dunet is not giving promising results.
*** integrate this with adv training

** Adv training of GANs?
** Compare with adv training
- show that the performance drop is not significant.
*** Try cifar10 challenge code
- model
- data augmentation
- PGD with their iteration
- CW by using CW loss function but PGD iterations

** investigate not only accuracy, but also confidence
** save keras training history


* Other Ideas                                                      :noexport:
** Ensemble
** random CNN as task


** TODO Add data augmentation during AE and adv training?
** Add noise, and then add PGD, and then use in training
** TODO add a little CW into PGD training
** unsuperwisely train AE
Do not use image data at all. Generate a data, assign random labels,
train the network. The network might have random guessing for
test/validation data, but can be 100% at training data. 

Using this network, train the AE.

* Additional Experiments
** DONE Black box substitute model accuracy
   CLOSED: [2019-05-21 Tue 11:33]
** DONE Model transfer
   CLOSED: [2019-05-21 Tue 12:15]
*** DONE Simple CNNs for MNIST
    CLOSED: [2019-05-16 Thu 00:28]
*** CANCELED VGG for CIFAR
    CLOSED: [2019-05-21 Tue 01:12]
*** DONE DenseNet
    CLOSED: [2019-05-21 Tue 11:33]
- original torch https://github.com/liuzhuang13/DenseNet
- keras implementation: https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet
** DONE DefenseGAN break
   CLOSED: [2019-05-21 Tue 01:12]
** DONE Test using all test data
   CLOSED: [2019-05-21 Tue 11:33]
instead of random 100

** TODO try other auto encoders other than dunet

* Nice-to-have experiments

** TODO Adv train both AE and CNN
** TODO use data augmentation during adv training

* Experiment

** TODO train on several digits, leave out 2
Do it on both AdvAE and adv training. This may even show better
performance than adv training.


** DONE CIFAR
   CLOSED: [2019-05-15 Wed 23:07]
*** TODO resnet AE design
*** TODO add high level xent when pretraining AE
*** TODO VGG etc for CIFAR
Because training AE for CIFAR is pretty hard
** TODO Imagenet

** TODO compare with other defenses
*** DONE Adv training
    CLOSED: [2019-05-15 Wed 23:07]
*** HAE: high-level feature guided AE
**** one iteration high adv prove it fail on white box
  - oblivious
  - unet
*** Ensemble method

*** TODO Compare to generative models
analyze the difference, pros and cons, compared to generative methods.
- Defense-GAN
- PuVAE


** AdvAE against different attacks
- test whether this works for CW

PostNoisy_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.13 |
| PGD     |     0.94 |          5.20 |
| JSMA    |     0.89 |          4.54 |
| CW      |     0.22 |          2.48 |

AdvAE (default) (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.10 |
| PGD     |     0.91 |          5.29 |
| JSMA    |     0.72 |          4.82 |
| CW      |     0.73 |           0.9 |

Post_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.97 |          6.10 |
| PGD     |     0.96 |          5.10 |
| JSMA    |     0.93 |          4.20 |
| CW      |     0.57 |           0.9 |

*** TODO we need a total accuracy table

|      | AdvAE | PostNoisy_Adv | AdvAE (10 epoch) |
|------+-------+---------------+------------------|
| FGSM |       |               |                  |
| PGD  |       |               |                  |
| JSMA |       |               |                  |
| CW   |       |               |                  |

*** TODO run full training instead of 10 epochs

** TODO AdvAE transferability to other CNN architectures

- test whether this works for different CNN structure out of box, or
  even FC

different CNN architecture:
- different kernel filter size
- different number of layers
- different activation functions
- different pooling size and scheme
- residual connections
- dropout

*** TODO Ensemble training
- ensemble different CNN architecture. I suspect that the rec terms
  actually act as regularizer for different CNNs. We'll see.

How to ensemble? Create many CNN layers. When training, add all loss
terms of different CNNs together.

** Ensemble different attack parameters
Or random

** TODO Analyze of different loss terms
- [ ] plot the training and loss
- analyze how the different loss terms work. Even if the loss does
  not seem to decrease, it might act as a regularizer. Try removing it
  in the train step, and observe if that term increases and goes out
  of control.
- see whether it is necessary any more to use high layers of CNN.
- add weights to the different terms, and apply weight decay

|   | term1 | term2 | term3 | term4 | adv accuracy |
|---+-------+-------+-------+-------+--------------|
|   | Y     |       |       |       |              |
|   |       | Y     |       |       |              |
|   |       |       | Y     |       |              |
|   |       |       |       | Y     |              |
|---+-------+-------+-------+-------+--------------|
|   | Y     | Y     |       |       |              |
|   | Y     |       | Y     |       |              |


default model
- =AdvAE=

stand alone model (not likely to work)
- =Post=

combine witth adv loss
- =Post_Adv=
- =Noisy_Adv=
- =PostNoisy_Adv=

add clean models
- =CleanAdv=
- =Post_CleanAdv=
- =Noisy_CleanAdv=
- =PostNoisy_CleanAdv=

high-level guided models
- High
- =High_Adv=
- =PostHigh_Adv=

** Denoiser capacity
- investigate whether increasing denoiser capacity helps with defense
  against CW
- test whether using FC instead of AE can also achieve similar results
** visualize what the denoiser is doing on adv images
** TODO visualize and analyze the successful attacks

** TODO PostAdv
- add adv noise at CNN input, after AE
- AE acts as a anti-adv example generator

* Result

MNIST (A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.98 |            |            0.98 |          |          0.97 |                   0.99 |        |
| CW      |         0. |       0.97 |            0.81 |     0.96 |            0. |                   0.86 |   0.55 |
| FGSM    |       0.16 |       0.95 |            0.95 |     0.98 |          0.24 |                   0.97 |        |
| PGD     |       0.01 |       0.96 |            0.94 |     0.99 |          0.02 |                   0.95 |        |

F-MNIST (A2)
| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.94 |            |            0.72 |          |          0.70 |                   0.83 |        |
| CW      |          0 |       0.72 |            0.45 |     0.74 |           0.0 |                   0.66 |        |
| FGSM    |       0.07 |       0.80 |            0.81 |     0.80 |          0.32 |                   0.83 |        |
| PGD     |       0.03 |       0.78 |            0.73 |     0.96 |          0.21 |                   0.69 |        |

F-MNIST (C0 A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN |
|---------+------------+------------+-----------------+----------+---------------+------------------------+--------|
| clean   |       0.94 |            |            0.82 |          |          0.70 |                   0.83 |        |
| CW      |          0 |       0.81 |            0.52 |     0.74 |           0.0 |                   0.66 |        |
| FGSM    |       0.07 |       0.76 |            0.72 |     0.80 |          0.32 |                   0.83 |        |
| PGD     |       0.03 |       0.78 |            0.63 |     0.96 |          0.21 |                   0.69 |        |

AdvAE Cifar10 (C0 A2)

| attacks | No defense | AdvAE obli | AdvAE white-box | HGD obli | HGD white-box | adv training white-box | DefGAN  |
|---------+------------+------------+-----------------+----------+---------------+------------------------+---------|
| clean   |       0.89 |            |            0.61 |          |          0.82 |                   0.67 |         |
| CW      |          0 |       0.62 |            0.01 |     0.82 |            0. |                     0. |         |
| FGSM    |       0.17 |       0.62 |            0.52 |     0.84 |          0.15 |                   0.48 |         |
| PGD     |       0.07 |       0.61 |            0.46 |     0.83 |          0.11 |                   0.43 |         |

Notes:
- HGD: B2 loss
- AdvAE MNIST: A2 loss
- AdvAE Cifar10: C0_A2 loss
- adv training: IdentityModel


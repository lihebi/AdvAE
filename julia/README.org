#+TITLE: Julia Implementation

* TODO-list

** TODO CIFAR10 models

*** TODO data augmentation
*** TODO densenet
https://github.com/FluxML/Metalhead.jl/
*** TODO learning rate schedule/decay
*** TODO early stopping


*** DONE batchnorm layer
    CLOSED: [2019-10-31 Thu 16:03]
*** DONE ResNet
    CLOSED: [2019-10-31 Thu 12:15]
*** CANCELED VGG
    CLOSED: [2019-10-31 Thu 12:15]

** TODO adversarial attacks
https://github.com/jaypmorgan/Adversarial.jl

*** DONE PGD
    CLOSED: [2019-11-01 Fri 16:27]
*** DONE FGSM
    CLOSED: [2019-11-01 Fri 16:27]
*** TODO CW
*** TODO black-box substitute model
*** TODO BoundaryAttack, BAPP
*** TODO BPDA

** TODO adversarial training
*** DONE itadvtrain
    CLOSED: [2019-11-01 Fri 16:27]

**** TODO convergency problem
There seems to be some problems: when directly using 20-PGD or 40-PGD, it does
not converge. Two ways:
- use 7-PGD, then 20-PGD, then 40-PGD. This seems to be the best strategy
- use 40-PGD directly, but train with both adv_x and x
- first train clean CNN for 1 epoch, then adv train
- weights initialization and regularization?

**** TODO consistency with python code
- speed seems to be a lot slower
- accuracy does not seem to be equal at each epoch
- convergency (or not) rate
- the final performance, 40-iter PGD, 0.8033, while should be 0.95

I'm going to use docker container to run the python code. For that I'd build a
machine with VNC support, via either:
- build ontop of tf official images
- see how tf official images are built, and build on top of ubuntu from scratch

*** STARTED AdvAE
**** auto encoder
***** Dunet
*** HGD

** STARTED GAN
*** GAN
*** DefenseGAN


** CANCELED fix the local package version problem
   CLOSED: [2019-11-02 Sat 13:28]


* DONE-list

** DONE @progress
   CLOSED: [2019-10-17 Thu 16:17]

ProgressMeter.jl https://github.com/timholy/ProgressMeter.jl

Very easy to use:

#+BEGIN_SRC julia
@showprogress 1 "Computing..." for i in 1:50
    sleep(0.1)
end
#+END_SRC


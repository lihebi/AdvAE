#+TITLE: Adversarially trained denoising Auto-Encoder (AdvAE)
#+LATEX_CLASS: nips
#+LATEX_HEADER: \usepackage[export]{adjustbox}

# These two combo can make larger width image while centered
# #+ATTR_LATEX: :width 1.2\linewidth,center

* Introduction
* Thread Model

* Approach

** Loss
We use the addition of four loss terms as loss function.

** Training
4. (optional) alternatively train denoiser and CNN, so that
the precision is still good. This may have equivalent effect as
training denoiser using high level feature guidance

4.1 FIXME probably also consider training for from clean x to x and to
logits, as that is the whole model

* Implementations notes                                            :noexport:
** DONE debug training time
   CLOSED: [2019-04-30 Tue 17:42]
** TODO inconsistency problems

- standalone attacks vs. integrated (in class as method) attacks: running time, accuracy
- accuracy computation inconsistency

** TODO CW visual result
** TODO add postadv baseline

* Other Ideas                                                      :noexport:
** Add noise, and then add PGD, and then use in training
** TODO add a little CW into PGD training

* Experiment
** TODO train on several digits, leave out 2
Do it on both AdvAE and adv training. This may even show better
performance than adv training.

** TODO test on CIFAR and ImageNet
** TODO compare with other defenses
- HAE: high-level feature guided AE
- Adv training

*** TODO Compare to generative models
analyze the difference, pros and cons, compared to generative methods.
- Defense-GAN
- PuVAE


** AdvAE against different attacks
- test whether this works for CW

PostNoisy_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.13 |
| PGD     |     0.94 |          5.20 |
| JSMA    |     0.89 |          4.54 |
| CW      |     0.22 |          2.48 |

AdvAE (default) (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.96 |          6.10 |
| PGD     |     0.91 |          5.29 |
| JSMA    |     0.72 |          4.82 |
| CW      |     0.73 |           0.9 |

Post_Adv (10 epochs)

| attacks | accuracy | l2-distortion |
|---------+----------+---------------|
| FGSM    |     0.97 |          6.10 |
| PGD     |     0.96 |          5.10 |
| JSMA    |     0.93 |          4.20 |
| CW      |     0.57 |           0.9 |

*** TODO we need a total accuracy table

|      | AdvAE | PostNoisy_Adv | AdvAE (10 epoch) |
|------+-------+---------------+------------------|
| FGSM |       |               |                  |
| PGD  |       |               |                  |
| JSMA |       |               |                  |
| CW   |       |               |                  |

*** TODO run full training instead of 10 epochs

** TODO AdvAE transferability to other CNN architectures

- test whether this works for different CNN structure out of box, or
  even FC

different CNN architecture:
- different kernel filter size
- different number of layers
- different activation functions
- different pooling size and scheme
- residual connections
- dropout

*** TODO Ensemble training
- ensemble different CNN architecture. I suspect that the rec terms
  actually act as regularizer for different CNNs. We'll see.

How to ensemble? Create many CNN layers. When training, add all loss
terms of different CNNs together.

** TODO Analyze of different loss terms
- [ ] plot the training and loss
- analyze how the different loss terms work. Even if the loss does
  not seem to decrease, it might act as a regularizer. Try removing it
  in the train step, and observe if that term increases and goes out
  of control.
- see whether it is necessary any more to use high layers of CNN.
- add weights to the different terms, and apply weight decay

|   | term1 | term2 | term3 | term4 | adv accuracy |
|---+-------+-------+-------+-------+--------------|
|   | Y     |       |       |       |              |
|   |       | Y     |       |       |              |
|   |       |       | Y     |       |              |
|   |       |       |       | Y     |              |
|---+-------+-------+-------+-------+--------------|
|   | Y     | Y     |       |       |              |
|   | Y     |       | Y     |       |              |


default model
- =AdvAE=

stand alone model (not likely to work)
- =Post=

combine witth adv loss
- =Post_Adv=
- =Noisy_Adv=
- =PostNoisy_Adv=

add clean models
- =CleanAdv=
- =Post_CleanAdv=
- =Noisy_CleanAdv=
- =PostNoisy_CleanAdv=

high-level guided models
- High
- =High_Adv=
- =PostHigh_Adv=

** Denoiser capacity
- investigate whether increasing denoiser capacity helps with defense
  against CW
- test whether using FC instead of AE can also achieve similar results
** visualize what the denoiser is doing on adv images
** TODO visualize and analyze the successful attacks

** TODO PostAdv
- add adv noise at CNN input, after AE
- AE acts as a anti-adv example generator

* Related Work
* Conclusion

* Appendix

# #+CAPTION: AdvAE
# [[./images/AdvAE-training-process-split.pdf]]
# #+CAPTION: Post
# [[./images/Post-training-process-split.pdf]]

# #+CAPTION: =Post_Adv=
# [[./images/Post_Adv-training-process-split.pdf]]
# #+CAPTION: =Noisy_Adv=
# [[./images/Noisy_Adv-training-process-split.pdf]]
# #+CAPTION: =PostNoisy_Adv=
# [[./images/PostNoisy_Adv-training-process-split.pdf]]

# #+CAPTION: CleanAdv
# [[./images/CleanAdv-training-process-split.pdf]]
# #+CAPTION: =Post_CleanAdv=
# [[./images/Post_CleanAdv-training-process-split.pdf]]
# #+CAPTION: =Noisy_CleanAdv=
# [[./images/Noisy_CleanAdv-training-process-split.pdf]]
# #+CAPTION: =PostNoisy_CleanAdv=
# [[./images/PostNoisy_CleanAdv-training-process-split.pdf]]

# #+CAPTION: High
# [[./images/High-training-process-split.pdf]]
# #+CAPTION: =High_Adv=
# [[./images/High_Adv-training-process-split.pdf]]
# #+CAPTION: =PostHigh_Adv=
# [[./images/PostHigh_Adv-training-process-split.pdf]]

# [[./images/PostNoisy_Adv_Rec-training-process-split.pdf]]
